#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ä» PDB æå–æŒ‡å®šé“¾çš„æ°¨åŸºé…¸åºåˆ—ï¼ˆæ ‡å‡†æ®‹åŸºã€æœ‰ CA åŸå­ï¼‰ï¼Œ
åŒæ—¶ç”Ÿæˆ PDB æ®‹åŸºç¼–å·(å«æ’å…¥ç ) â†’ åºåˆ—ç´¢å¼•(0-based) çš„æ˜ å°„è¡¨ã€‚
è‹¥é“¾é•¿åº¦ > ESM-1b ä¸Šé™ (1022)ï¼Œç›´æ¥è·³è¿‡å¹¶è®°å½•åŸå› ã€‚

è¾“å…¥:
- data/filtered_abbind_single_mutations.csv   (éœ€å«åˆ—: #PDB, Mutation)
- structures/*.pdb                            (æ¯ä¸ª PDB ä¸€ä¸ªæ–‡ä»¶)

è¾“å‡º:
- data/wt_antibodies.fasta
- data/residue_index_maps/<PDB>_<CHAIN>.json  (é”®: "resseq|icode"ï¼Œä¾‹ "100|A" or "487|")
- results/extract_report.csv                  (ä¿ç•™/è·³è¿‡æ˜ç»†)
"""

import os
import json
import pandas as pd
from Bio.PDB import PDBParser
from Bio.SeqUtils import seq1
from Bio.Seq import Seq
from Bio.SeqRecord import SeqRecord
from Bio import SeqIO

# ================== é…ç½® ==================
PROJ = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
CSV_PATH = os.path.join(PROJ, "data", "filtered_abbind_single_mutations.csv")
PDB_DIR  = os.path.join(PROJ, "structures")
OUT_FASTA = os.path.join(PROJ, "data", "wt_antibodies.fasta")
MAP_DIR   = os.path.join(PROJ, "data", "residue_index_maps")
REPORT_CSV = os.path.join(PROJ, "results", "extract_report.csv")
MAX_LEN = 1022   # ESM-1b å•æ¬¡å¯å®‰å…¨å¤„ç†çš„æœ€å¤§é•¿åº¦ï¼ˆå»æ‰CLS/EOSåï¼‰

os.makedirs(os.path.join(PROJ, "data"), exist_ok=True)
os.makedirs(os.path.join(PROJ, "results"), exist_ok=True)
os.makedirs(MAP_DIR, exist_ok=True)

# ================== è¯»å–éœ€è¦çš„ PDB+é“¾ ==================
if not os.path.isfile(CSV_PATH):
    raise FileNotFoundError(f"æ‰¾ä¸åˆ°çªå˜CSV: {CSV_PATH}")

df = pd.read_csv(CSV_PATH)
if "#PDB" not in df.columns or "Mutation" not in df.columns:
    raise KeyError("CSV ä¸­éœ€è¦åŒ…å«åˆ—ï¼š#PDB å’Œ Mutation")

pairs = set()
for pdb, mut in zip(df["#PDB"], df["Mutation"]):
    if not isinstance(mut, str):
        continue
    pdbid = str(pdb).upper().strip()
    chain = mut.split(":")[0].strip()
    if pdbid and chain:
        pairs.add((pdbid, chain))

if not pairs:
    raise RuntimeError("æ²¡æœ‰ä» CSV ä¸­è§£æåˆ°ä»»ä½• PDB+é“¾ï¼›è¯·æ£€æŸ¥ CSV å†…å®¹ã€‚")

# ================== æå–åºåˆ— & ç”Ÿæˆæ˜ å°„ ==================
parser = PDBParser(QUIET=True)
records = []
report_rows = []

kept, skipped = 0, 0

for pdbid, chain_id in sorted(pairs):
    pdb_path = os.path.join(PDB_DIR, f"{pdbid}.pdb")
    status = "kept"
    reason = ""
    length = 0

    if not os.path.isfile(pdb_path):
        status, reason = "skipped", f"missing_pdb:{pdb_path}"
        length = 0
        report_rows.append({
            "pdb": pdbid, "chain": chain_id, "status": status, "reason": reason, "length": length
        })
        skipped += 1
        continue

    try:
        structure = parser.get_structure(pdbid, pdb_path)
        if 0 not in structure:
            status, reason = "skipped", "no_model_0"
            report_rows.append({"pdb": pdbid, "chain": chain_id, "status": status, "reason": reason, "length": 0})
            skipped += 1
            continue

        model = structure[0]
        if chain_id not in model:
            status, reason = "skipped", "chain_not_found"
            report_rows.append({"pdb": pdbid, "chain": chain_id, "status": status, "reason": reason, "length": 0})
            skipped += 1
            continue

        chain = model[chain_id]

        seq_chars = []
        num2idx = {}  # é”®: "resseq|icode" (icode ä¸ºç©ºæ—¶ç”¨ "")
        idx = 0

        for res in chain:
            hetfield, resseq, icode = res.id  # (hetero flag, residue sequence number, insertion code)
            # åªä¿ç•™æ ‡å‡†æ°¨åŸºé…¸å¹¶ç¡®ä¿æœ‰ CA åŸå­ï¼ˆè¿‡æ»¤æ°´åˆ†å­ã€é…ä½“ã€ç¼ºå¤±/ä¸å®Œæ•´æ®‹åŸºï¼‰
            if hetfield == " " and "CA" in res:
                aa1 = seq1(res.get_resname())
                # é‡åˆ°æœªçŸ¥/éæ ‡å‡†æ®‹åŸºåæ—¶ï¼Œseq1 å¯èƒ½è¿”å› "X"ï¼›è¿™é‡Œç›´æ¥ä¿ç•™Xæˆ–è·³è¿‡éƒ½å¯ä»¥
                # ä¸ºäº†ä¸ç ´åç´¢å¼•ï¼Œæˆ‘ä»¬ä¿ç•™å®ƒ
                seq_chars.append(aa1)
                key = f"{int(resseq)}|{(icode or '').strip()}"
                num2idx[key] = idx
                idx += 1

        seq_str = "".join(seq_chars)
        length = len(seq_str)

        # è·³è¿‡è¿‡é•¿çš„é“¾ï¼ˆ> MAX_LENï¼‰
        if length > MAX_LEN:
            status, reason = "skipped", f"too_long:{length}>{MAX_LEN}"
            report_rows.append({"pdb": pdbid, "chain": chain_id, "status": status, "reason": reason, "length": length})
            skipped += 1
            continue

        # å†™å…¥ FASTA & æ˜ å°„
        rec_id = f"{pdbid}_{chain_id}"
        records.append(SeqRecord(Seq(seq_str), id=rec_id, description=""))
        with open(os.path.join(MAP_DIR, rec_id + ".json"), "w") as f:
            json.dump(num2idx, f)

        kept += 1
        report_rows.append({"pdb": pdbid, "chain": chain_id, "status": status, "reason": reason, "length": length})

    except Exception as e:
        status, reason = "skipped", f"parse_error:{type(e).__name__}:{e}"
        report_rows.append({"pdb": pdbid, "chain": chain_id, "status": status, "reason": reason, "length": 0})
        skipped += 1

# å†™ FASTA
if records:
    SeqIO.write(records, OUT_FASTA, "fasta")
else:
    # å³ä¾¿æ²¡æœ‰ä¿ç•™é“¾ï¼Œä¹Ÿç»™ä¸€ä¸ªç©ºæ–‡ä»¶ï¼Œé¿å…åç»­è„šæœ¬æ‰¾ä¸åˆ°
    open(OUT_FASTA, "w").close()

# å†™æŠ¥è¡¨
pd.DataFrame(report_rows).to_csv(REPORT_CSV, index=False)

print("==============================================")
print(f"âœ… æå–å®Œæˆï¼šä¿ç•™ {kept} æ¡ï¼Œè·³è¿‡ {skipped} æ¡")
print(f"ğŸ“„ FASTAï¼š {OUT_FASTA}")
print(f"ğŸ—ºï¸ æ˜ å°„ï¼š {MAP_DIR}/*.json")
print(f"ğŸ§¾ æŠ¥è¡¨ï¼š {REPORT_CSV}")
print("==============================================")
